# llmcompareFramework
A Python framework for benchmarking multiple Large Language Models (LLMs) across various NLP datasets and tasks â€” including MMLU, SQuAD, TruthfulQA, RACE, CNN/DailyMail, and more. Supports OpenAI, Ollama, and Anthropic model providers.
